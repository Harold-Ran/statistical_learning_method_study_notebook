[TOC]
**导读**：

- **聚类**：依据样本特征的**相似度或距离**，将其归并到若干个**“类”或“簇”**的数据分析问题
- **目的**：通过得到的类或簇来发现数据的特点或对数据进行处理。
- **聚类**：属于无监督学习，因为只是根据样本的相似度或距离将其进行归类，而类或簇事先并不知道
- 本章主要介绍：**层次聚类hierarchical clustering**，**k均值聚类k-means clustering**。

# 1. 聚类的基本概念
## 1.1 相似度或距离
**聚类**的核心概念就是**相似度similarity**或**距离distance**。有多种相似度和距离的定义，且相似度直接影响聚类结果。
- **闵可夫斯基距离 Minkowski distance**：在聚类中，可将**样本集合**看作**向量空间**中点的集合，以向量空间中的**距离**表示样本之间的**相似度**。

| 相似度度量方法     |                             函数                             |
| :----------------- | :----------------------------------------------------------: |
| Minkowski distance | d_{ij} = (\sum_{k=1}^N{ \vert x_{ki}-x_{kj} \vert }^p )^{\frac{1}{p}} |
| Euclidean distance | $d_{ij} = (\sum_{k=1}^N{ \vert x_{ki}-x_{kj} \vert }^2 )^{\frac{1}{2}}$ |
| Manhattan distance |   $d_{ij} = (\sum_{k=1}^N{ \vert x_{ki}-x_{kj} \vert } )$    |
| Chebyshev distance | $d_{ij} = ( \underset{k}{max}  \vert x_{ki}-x_{kj} \vert )$  |

- Minkowski距离就是$L_{P}$范数$(P>=1)$，而 Manhattan 距离、Euclidean距离、Chebyshev距离分别对应$P=1,2,+\infty$时的情形。

---
- **马哈拉诺比斯距离 Mahalanobis distance**:简称马氏距离，考虑各个分量（特征）之间的相关性，与各个分量的尺度无关，距离越大，相似度越小。S为协方差矩阵。
$$d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^{\frac{1}{2}}$$
当S为单位矩阵时，样本数据各个分量互相独立，且各个分量的方差为1。

---
**相关系数correlation coefficient**：可以表达样本之间的相似度,相关系数的绝对值越接近1，表示样本越相似。

$$r_{ij} = \frac{ \sum_{k=1}^m(x_{ki} - \bar{x}_i)(x_{kj} - \bar{x}_j) }{[ \sum_{k=1}^m(x_{ki} - \bar{x}_i)^2 \sum_{k=1}^m(x_{kj} - \bar{x}_j)^2 ]^\frac{1}{2} }$$


---
**夹角余弦 cosine**：可以表达样本相似度，夹角余弦越接近1，表示样本越相似。
$$s_{ij} = \frac{ \sum_{k=1}^m x_{ki}x_{kj} }{[ \sum_{k=1}^mx_{ki}^2  \sum_{k=1}^mx_{kj}^2]^\frac{1}{2}}$$

![在这里插入图片描述](https://img-blog.csdnimg.cn/9444c1f83f8e4627b084ad093a8d11bc.png)
- 选取合适的相似度函数：如上图所示：不同的函数，聚类的结果不同
- 从距离的角度看，A和B比A和C更相似
- 从相关系数的角度看，A和C比A和B更相似

## 1.2 类或簇
- 聚类得到的类或簇，本质是样本的子集。根据子集有无交集分为**硬聚类**（hard clustering）和**软聚类（soft clustering）**。
- **类的特征**：
	- 类的均值，中心$\bar{x}_G$ ：$\bar{x}_G = \frac{1}{n_G} \sum_{i=1}^{n_G} x_i$
	- 类的直径：$D_G = \underset{x_i,x_j}{max} d_{ij}$
	- 类的样本散布矩阵scatter matrix：$A_G = \sum_{i=1}^{n_G}(x_i- \bar{x}_G)(x_i - \bar{x}_G)^T$
	- 样本协方差矩阵$S_G$:  $S_G = \frac{1}{m-1}A_G = \frac{1}{m-1}  \sum_{i=1}^{n_G}(x_i- \bar{x}_G)(x_i - \bar{x}_G)^T$
	
## 1.3 类之间的距离
- 最短距离或单连接single linkage:定义类$G_p$的样本与$G_q$的样本之间最短的距离为两类的距离 
- 最长距离或完全连接complete linkage:定义最长距离为两类的距离
- 中心距离:定义类中心之间的距离为两类距离
- 平均距离:定义类$G_p$与类$G_q$任意两样本之间的距离平均值为两类的距离


# 2. 层次聚类
- **层次聚类**：假设类别之间 存在层次结构，将样本聚到层次化的类中。层次聚类属于硬聚类，分为以下两类：
	- 聚合（agglomerative）或自下而上（bottom-up）聚类
	- 分裂（divisive）或自上而下（top-down）聚类
- **聚合聚类**：
	-  将每个样本各自分到一个类
	- 之后将相距最近的两类合并，建立一个新的类，重复此操作直到满足停止条件
	- 得到层次化的类别
- **分裂聚类**：
	- 将所有样本分到一个类
	- 将已有类中相距最远的样本分到两个新的类，重复上一步直到满足停止条件
	- 得到层次化的类别
- 聚合聚类的**步骤**：
	- 对给定的样本集合，开始将每个样本分到一个类
	- 按照一定规则，例如 类间距离最小，将最满足规则条件的两个类进行合并
	- 反复上一步，每次减少一个类，直到满足停止条件，如所有样本聚为一类
- 聚合聚类**三要素**：
	-  距离或相似度（闵可夫斯基距离、马哈拉诺比斯距离、相关系数、夹角余弦）
	- 合并规则（类间距离最小，可以是最短距离、最长距离、中心距离、平均距离）
	- 停止条件（类的个数达到阈值（极端情况类的个数是1）、类的直径超过阈值）

- **算法14.1**：
- 输入：$n$个样本组成的集合$X$
- 输出：对样本的一个层次化聚类$C$
	- 计算$n$个样本两两之间的欧氏距离${d_{ij}}$，记作矩阵$D=[d_{ij}]_{n\times n}$
	- 构造$n$个类，每个类只包含一个样本
	- 合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。
	- 计算新类与当前各类之间的距离。如果类的个数是1， 终止计算，否则回到步骤3。

- 算法复杂度比较为$O(n^3m)$

![在这里插入图片描述](https://img-blog.csdnimg.cn/05843699f16541c1a1e8e0de28a0c5ff.png)

# 3. K均值聚类
**k均值聚类**：是基于样本集合划分的聚类算法
- 将样本集合划分为 k 个子集，构成 k 个类
- 将 n 个样本分到 k 个类中，每个样本到其所属类的中心的距离最小
- 每个样本只能属于一个类，是**硬聚类**

## 3.1 模型
K均值聚类的目标：是将n个样本分到k个不同的类或簇中

## 3.2 策略
策略：通过损失函数的最小化选取最优的划分或函数C*
- **样本距离**：**欧式距离平方** $d(x_i,x_j) = \sum_{k=1}^m (x_{ki} - x_{kj} )^2$ 
- **损失函数**：样本与所属类的中心之间的距离的总和：$W(C) = \sum_{l=1}^k \sum_{C(i)=l} \left \| x_i - \bar{x}_l \right \|^2$
- K均值聚类就是**求解最优化问题**： $C^* = \underset{C}{argmin} W(C) = \underset{C}{argmin} \sum_{l=1}^k \sum_{C(i)=l} \left \| x_i - \bar{x}_l \right \|^2$

## 3.3 算法
k均值聚类的算法是迭代的过程，每次迭代包括两个步骤：
- 首先随机选择 k 个类的中心（选 k 个样本），将其余样本逐个指派到与其最近的中心的类中，得到一个聚类结果
- 然后更新每个类的样本的均值，作为类的新的中心
- 重复以上步骤，直到收敛

- **算法14.2**：
- 输入：$n$个样本的集合$X$
- 输出：样本集合的聚类$C^*$
	- 初始化。
	- 对样本进行聚类。
	- 计算类的中心。
	- 如果迭代收敛或符合停止条件，输出$C^*=C^{(t)}$


## 3.4 算法特性
- 1，**总体特点**
	- 基于划分的聚类方法
	- 类别数 k 事先指定
	- 以**欧氏距离平方**表示样本之间的距离
	- 以中心或样本的均值表示类别
	- 以 样本和其所属类的中心 之间的 距离的总和 为最优化目标函数
	- 得到的类别是平坦的、非层次化的
	- 是迭代算法，不能保证得到全局最优

- 2，**收敛性**
	- k均值聚类属于启发式方法，不能保证收敛到全局最优
	- 初始中心的选择会直接影响聚类结果
	- 类中心在聚类的过程中会发生移动，但是往往不会移动太大，因为在每一步，样本被分到与其最近的中心的类中

- 3，**初始类的选择** 
	- 选择不同的初始中心，会得到不同的聚类结果
	- 初始中心的选择，比如 可以用层次聚类对样本进行聚类，得到k个类时停止。然后从每个类中选取一个与中心距离最近的点

- 4， **类别数k的选择**
- k 值需要预先指定，而在实际应用中最优k值是不知道的
- 解决方法：尝试不同的k值，检验聚类的质量，推测最优的k值
- 聚类结果的质量：可以用类的平均直径来衡量
- 一般地，类别数变小时，平均直径会增加；类别数变大超过某个值以后，平均直径会不变；而这个值正是最优的k值
- 可以采用二分查找，快速找最优的k值


## 3.5 实例解释

k-means，下左图是原始数据集，通过观察发现大致可以分为4类，所以取k=4，测试数据效果如下右图所示。
![在这里插入图片描述](https://img-blog.csdnimg.cn/0d49398d8dcb4f98ab53cc0fa74de6ee.png)
当时初始质心点取值不同的时候，最终的聚类效果也不一样，接下来我们看一个具体的实例：
![在这里插入图片描述](https://img-blog.csdnimg.cn/7758f8a8018e4e9396d3ccde89713183.png)
这个例子当中，下方的数据应该归为一类，而上方的数据应该归为两类，这是由于初始质心点选取的不合理造成的误分。而k值的选取对结果的影响也非常大，同样取上图中数据集，取k=2,3,4，可以得到下面的聚类结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/289a7bb29be948a7a196d0271629c59c.png)
因此，k-means算法：
- 需要提前确定值
- 对初始质心点敏感
- 对异常数据敏感

更多聚类方法可参考：
**参考**：
李航，统计学习方法第二版
常用聚类算法：https://zhuanlan.zhihu.com/p/104355127
层次聚类算法：https://zhuanlan.zhihu.com/p/363879425
sklearn介绍的10中聚类算法：https://scikit-learn.org/stable/modules/clustering.html