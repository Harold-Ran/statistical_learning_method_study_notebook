# 第6章 逻辑斯谛回归与最大熵模型

## 1 逻辑斯谛回归模型

前面我们学过感知机是通过线性分离超平面$y=\omega x+b$来进行二分类的，简单来说就是用超平面把整个空间一分为二，一边是正例另一边是负例。但是我们对那些和超平面距离比较近的样本点并不那么自信，如样本$x_1$分类为正例的可能性只有51%，而和超平面距离较远的那些样本点则可以认为它的分类很大概率是正确的，如样本$x_2$分类为负例的可能性是99%，那么要怎么把样本点离超平面的距离转化成它属于当前标签的概率呢？人们找到了逻辑斯谛函数。

![chap6-线性分类模型](images\chap6-线性分类模型.png)

逻辑斯谛函数是一条S形曲线（sigmoid curve），其分布函数表达式如下，它是一条以点$(\mu, \frac{1}{2})$中心对称的曲线。
$$
F(x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}
$$
<img src="images\chap6-逻辑斯谛分布.png" alt="chap6-逻辑斯谛分布" style="zoom:40%;" />

如果我们令$\mu=0，\gamma=1$，则函数就变成$F(x)=\frac{1}{1+e^{-x}}$，称为sigmoid函数，许多同学对这个名字应该并不陌生，它就是当前在深度学习模型中广泛使用的一种激励函数。可以看到，曲线在$x=0$附近变化快，在两端变化慢，如果我们把样本$x$到超平面的距离$\frac{1}{||\omega||}|\omega x + b|$作为sigmoid函数的输入$x$，那么显然距离越大，函数的输出$y$越接近1，这不正是我们想要的能体现样本属于某一标签的概率吗？

![chap6-sigmoid函数曲线](images\chap6-sigmoid函数曲线.jpeg)

实际上我们并不直接把距离$\frac{1}{||\omega||}|\omega x + b|$作为函数的输入，而是把$\omega x + b$作为函数的输入，这是因为对于所有样本来说，$||\omega||$是相同的，因此把系数$\frac{1}{||\omega||}$去掉并不影响样本之间的相对距离，同时由于sigmoid函数是关于$(0, \frac{1}{2})$中心对称的，对于负例，$\omega x+b$为负值也同样能反映出距离越远（$\omega x+b$越小）样本属于正例的概率越小（函数的输出$y$越接近0）。因此我们把$\omega x + b$代入sigmoid函数，就得到了逻辑斯蒂回归模型的表达式。
$$
\begin{aligned}
P(Y=1|x)
&= \frac{1}{1+e^{-(\omega x + b)}} \\
&= \frac{e^{\omega x + b}}{1 + e^{\omega x + b}} \\
P(Y=0|x) 
&= 1 - P(Y=1|x) \\
&=\frac{1}{1 + e^{\omega x + b}}
\end{aligned}
$$
那么模型的参数$\omega$和$b$要怎么得到呢，对于这种概率模型，最常用的参数估计方法是极大似然估计法，或者是梯度下降法和拟牛顿法，这里我们就不过多赘述了。

上述模型只能用于二分类任务，如果要推广到多分类任务怎么办呢？了解深度学习的同学很容易能想到，采用softmax函数。同样将$\omega x + b$代入softmax函数，对于$Y$取值集合为$\{1, 2, ..., K\}$的K分类任务，得到多项逻辑斯谛回归模型如下
$$
P(Y=k|x) = \frac{e^{\omega_k x + b}}{1+\sum_{k=1}^{K-1}e^{\omega_kx+b}}, k=1, 2, ..., K-1 \\
P(Y=K|x)=1-\sum_{k=1}^{K-1}P(Y=k|x)=\frac{1}{1+\sum_{k=1}^{K-1}e^{\omega_kx+b}}
$$

## 2 最大熵模型

最大熵模型根据最大熵原理得到的分类模型。那什么是最大熵原理呢？在决策树中我们知道了，熵反应的是不确定程度，最大熵原理认为，在满足了全部已知条件的所有可能的模型中，熵最大的那个模型就是最优的模型，因为它的不确定性最大，这就说明它不带任何我们人为引入的主观假设，此时的模型是无偏的。

假设我们的分类模型是一个概率模型$P(Y|X)$，在给定训练集$T=\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$的情况下，可以根据训练集得到联合分布$P(X, Y)$的经验分布$\tilde{P}(X=x,Y=y)$就是满足$X=x$且$Y=y$的样本个数$\nu(X=x, Y=y)$除以样本总数$N$，边缘分布$P(X)$的经验分布$\tilde{P}(X=x)$就是满足$X=x$的样本个数$\nu(X=x)$除以样本总数$N$。
$$
\tilde{P}(X=x,Y=y)=\frac{\nu(X=x, Y=y)}{N} \\
\tilde{P}(X=x)=\frac{\nu(X=x)}{N}
$$
假设我们定义一个特征函数$f(x,y)$，当样本满足$X=x, Y=y$时$f(x,y)=1$，否则$f(x,y)=0$，那么对于训练集来说，关于联合分布的经验分布$\tilde{P}(X=x,Y=y)$，期望值为所有样本的特征函数$f(x,y)$乘上对应的经验分布$\tilde{P}(x,y)$之和，用$E_{\tilde{P}}(f)$表示。
$$
E_{\tilde{P}}(f)=\sum_{x,y}\tilde{P}(x,y)f(x,y)
$$
如果模型$P(Y|X)$能够学习到训练集的联合分布，根据条件概率与联合概率之间的变换关系就应该有$\tilde{P}(X=x,Y=y)=P(y|x)\tilde{P}(X=x)$，关于模型$P(Y|X)$和边缘分布的经验分布$\tilde{P}(X=x)$计算出来的期望值为
$$
E_P(f)=\sum_{x, y}\tilde{P}(x)P(y|x)f(x,y)
$$
在模型能够学习到训练集的联合分布的情况下，这两种方式计算出来的期望值就应该是相等的，即$E_{\tilde{P}}(f)=E_P(f)$。

模型$P(Y|X)$的条件熵为
$$
H(P)=-\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x)
$$
根据最大熵原理，最优的模型首先应该满足全部的已知条件，也就是$E_{\tilde{P}}(f)=E_P(f)$，并且模型的概率之和要为1，即$\sum_{y}P(y|x)=1$，在满足这两个条件的情况下，取熵$H(P)$最大的模型，因此求解最大熵模型可以表达为以下约束最优化问题，其中$n$为$(x,y)$的所有可能取值的个数：
$$
\begin{aligned}
& \max_{P\in C} \quad H(P) = -\sum_{x,y}\tilde{P}(x)P(y|x)logP(y|x) \\
& 
\begin{aligned}
s.t. \quad 
& E_P(f_i) = E_{\tilde{P}}f_i, \quad i=1, 2, ..., n \\
& \sum_yP(y|x) = 1
\end{aligned}
\end{aligned}
$$
这种最优化问题可以转换成对偶问题通过拉格朗日乘数法求解，求解过程这里不多赘述，最终可以得到最大熵模型为
$$
P_{\omega}(y|x)=\frac{1}{Z_{\omega}(x)}\exp(\sum_{i=1}^n\omega_i f_i{(x,y)}) \\
Z_{\omega}(x)=\sum_y \exp(\sum_{i=1}^n\omega_i f_i{(x,y)})
$$
从形式上看，最大熵模型与逻辑斯谛回归模型非常相似。假定标签$Y$的取值集合是$\{0, 1\}$，这是一个二分类任务，此时根据最大熵模型，有
$$
P(Y=1|x)=\frac{\exp(\sum_{i=1}^n\omega_if(x,1))}{\exp(\sum_{i=1}^n\omega_if(x,0)) + \exp(\sum_{i=1}^n\omega_if(x,1))}
$$
在输入为$x$且$Y=1$的情况下，特征函数$f(x,0)=0$，则
$$
P(Y=1|x)=\frac{\exp(\sum_{i=1}^n\omega_if(x,1))}{1 + \exp(\sum_{i=1}^n\omega_if(x,1))}
$$
这不就是二项逻辑斯谛回归模型的形式吗，但是区别在于，二项逻辑斯谛回归模型的权重$\omega_i$是仅针对输入变量$x$的，而最大熵模型中的权重$\omega_i$是针对特征函数$f(x,y)$的，同时考虑了输入$x$和输出$y$的取值。

