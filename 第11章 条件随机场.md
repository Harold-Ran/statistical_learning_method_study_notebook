---
title: Conditional Random Field
date: 2022-10-28 15:16:22
categories: Data Science
mathjax: true
---

关于概率图模型，在百度百科上的解释是**用图来表示变量概率依赖关系的理论**，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布。由于本文读者数学水平不同，而要想深究概率图模型需要的数学背景太多，因此本文作为李航老师的统计学习方法概率图模型一章的补充，将用尽量少（但不可避免）的数学公式为大家展现更直观的概率图模型。

# 一、背景介绍

为了要介绍概率图模型，我们需要了解CRF是如何通过隐马尔可夫模型（HMM）推出的，比起HMM的进化版——最大熵马尔可夫模型（MEMM）它又有什么改进，实际上该部分是理解CRF模型最重要的一环。在机器学习中，对于一个高维随机变量 $x_1, x_2, ..., x_p$ 而言，我们通常需要求其边缘概率 $p(x_i)$ 以及条件概率 $p(x_j|x_i)$，边缘概率我们通常使用 Sum Rule 求得，也就是
$$p(x_1)=\int p(x_1, x_2) \mathbf{d} x_2$$
对于条件概率我们通常利用贝叶斯定理：
$$p(x_2|x_1) = \frac{p(x_1, x_2)}{p(x_1)} = \frac{p(x_1, x_2)}{\int p(x_1, x_2) \mathbf{d} x_2}$$
我们会发现对于这两类问题我们都需要通过求解联合概率 $p(x_1, x_2, ..., x_p)$ 获得，但是联合概率的求解存在的问题是维度过高，导致计算量太大，因此为了简化计算，最开始人们提出的是相互独立假设，也即
$$p(x_1, ..., x_p) = \prod_{i=1}^p p(x_i)$$
一些著名的算法便利用了这一定理，例如Naive Bayes便假定 $p(x|y) = \prod_{i=1}^p p(x_i|y)$，但这个条件太强了，为了放松这一条件，齐次Markov假设则应运而生，即：在给定现在状态时，它与过去状态（即该过程的历史路径）是条件独立的。公式化为：
$$x_j\perp x_{i+1} | x_i, j<i$$
但这一假设的明显缺陷是依赖条件太过单一，有没有可能我是依赖于前两个时间步而不是前一个时间步呢，有没有可能是有更复杂的依赖关系呢，于是条件独立性假设作为马氏链的推广又被提出，也就是统计学习方法一书最开始提到的因子分解：
$$p(x_1, x_2, ..., x_p) = \prod_{i=1}^p p(x_i|x_{pa(i)})$$
其中 $pa(i)$ 是 $i$ 的父亲节点集合。


## 1. 硬分类和软分类

我们知道分类问题是机器学习主要探究的问题之一，而硬分类和软分类作为分类模型的两大类，又分别对应一个分类模型的输出值是否为一个概率。对于硬分类而言，最常见的模型有如SVM、PLA和LDA，这类模型的形式为决策函数，即为输入 $x$ 到输出 $y$ 的一个映射，且输出唯一。而软分类主要分为两大类，一类是概率判别模型（例如Logistic Regression模型），一类是概率生成模型（例如Naive Bayes和隐马尔可夫模型）。概率判别模型其实是沿用最大熵模型的思想，对 $P(y|x)$ 进行建模，而概率生成模型则对 $P(x, y)$ 进行建模。最大熵马尔可夫模型（MEMM）作为一个判别模型，则进一步结合了最大熵模型和隐马尔可夫模型的优点，打破了HMM的观测独立假设，但同时存在着著名的标注偏差问题。而条件随机场（CRF）的最重要贡献就是利用了无向图天然的全局归一化性质，将原来MEMM模型的有向图转为无向图，从而解决了标注偏差问题。


## 2. 隐马尔可夫模型到条件随机场的演变

隐马尔可夫模型的参数我们使用 $\lambda = (\pi, A, B)$ 来表达，其中 $\pi$ 为初始的状态分布， $A$ 为初始的状态转移矩阵，$B$ 为发射矩阵。隐马尔可夫模型为了考虑计算上的方便，我们会默认其满足两个条件（假设）：

1. 齐次一阶马尔可夫：下一时间点事件发生的概率只与上一时间点有关系，且转移概率与时间无关（为同一概率分布）
2. 观测独立假设：对于如下左图，即 $P(x_t|y_{1:t}, x_{1:t})=P(x_t|y_t)$

但这两种假设都不是必须的，都是一种折衷，比如我们知道一篇文章的单词之间当然是有关联的，不是相互独立的。因此对于MEMM模型将箭头改为了如下右图所示的相反方向，我们可以看到，给定 $y_t$ 情况下，$x_{t-1}$ 和 $x_{t-1}$ 显然不是独立的，由此打破了观测独立假设，并且将原模型从生成模型转为了判别模型。

![hmm_memm](https://cdn.staticaly.com/gh/chenkai66/Picture@main/hmm_memm.123irwf05luo.webp)

上面介绍的MEMM模型可以公式化为：
$$P(Y|X, \lambda) = \Pi_t P(y_t|y_{t-1}, x_{1...T}, \lambda)$$
然而该模型有一个看似很小的问题——labeling bias，但是在最坏的情况下，labeling bias会导致模型在预测下一个标签时完全忽略当前的观察结果。究其原因主要来自他的mass score：
$$P(I \mid O)=\prod_{t=1}^{n} \frac{\exp \left(\sum_{a}\right) \lambda_{a} f_{a}(o, i)}{Z\left(o, i_{i-1}\right)}, i=1, \cdots, n$$
在指数内部我们可以看到一个求和，这个求和的作用实际上是归一化（local 归一化），正是由于这个归一化的过程导致了MEMM的viterbi求解里的转移公式的第二部分出现了问题，这也导致dp无法正确的递归到全局的最优。我们可以把这个过程想象成一个绳子，我们抖动它的一段时产生的能量是会向后发散出去的，原本不同节点将会对能量做一个动态的调整，但归一化这个步骤将总能量立刻结算了。具体的例子如下：

![the_cat_sit](https://cdn.staticaly.com/gh/chenkai66/Picture@main/the_cat_sit.7j8vzob0vvg0.webp)

考虑 the cat sat 这个句子，首先我们计算 $[ARTICLE, NOUN, VERB]$ 的概率应为 $1.0×0.9×1.0$，但如果我们只考虑 cat sat 这个句子，其正确标签本应该是 $[NOUN, VERB]$，然而我们发现，它的结果为 $[ARTICLE, NOUN]$ 的概率为 $0.9×0.3$，而 $[NOUN, VERB]$ 的概率却相应的只有 $0.1*1.0$，这是因为该模型之前很少见到将cat作为第一个单词的例子，但事实上我们需要的是该模型对于给定观察和先前的标签的不确定性的信息，其实模型本来是有可能在某个时候隐含地存储了这种不确定性信息， 然而我们通过由于将观测数值进行了归一化，这些信息被我们直接丢弃了。例如对于一个没有标准化处理过的图而言：

![unm_the_cat_sit](https://cdn.staticaly.com/gh/chenkai66/Picture@main/unm_the_cat_sit.5jbhd04ley80.webp)

我们重新分析这个没有归一化的图，可以发现 cat 来自 $<S>$ 的概率原本就很小，导致其召回率很低。现在重新计算这个图，我们会发现 $[ARTICLE, NOUN]$ 的分数是 $5+21=26$ 分，而 $[NOUN, VERB]$ 的分数为 $3+100=103$ 分，这一表现比起归一化后的结果要好太多了~

我们可以总结如下：对于该条件转移概率分布，如果熵越小，结果越不关注obervation。而为了解决这一问题，我们最简单的处理方式就是将有向图变为无向图。

![crf](https://cdn.staticaly.com/gh/chenkai66/Picture@main/crf.6wh2s8cadpo0.webp)

# 二、CRF模型

## 1. Linear-CRF的参数化形式

对于无向图的最大团 $C$（最大团以及其他的定义可以参看李航老师的原书），对于概率无向图模型的联合概率分布  $P(Y)$  可以表示为如下形式:
$$\begin{aligned}
P(x) &=\frac{1}{Z} \prod_{C} \Psi_{C}\left(x_{c_i}\right) \\
&=\frac{1}{Z} \prod_{C}\mathbf{exp}[-E_i(x_{c_i})]  \\
&=\frac{1}{Z} \mathbf{exp}\sum_{C}[F_i(x_{c_i})]  \\
Z &=\sum_{x} \prod_{C} \Psi_{C}\left(Y_{C}\right)
\end{aligned}$$
其中 $\Psi_{C}$ 为势函数，$E_i(Y_C)$ 为能量函数。
根据CRF的概率图表示，对于线性链的概率密度函数可以根据上式做出简化：
$$\begin{aligned}
P(Y|X) &= \frac{1}{Z}\mathbf{exp}\sum_{i=1}^KF_i(x_{c_i})\\
&= \frac{1}{Z}\mathbf{exp}\sum_{t=1}^TF_i(y_{t-1}, y_t, x_{1:T})
\end{aligned}
$$
单独看线性链中的最大团，对于序列的第 $t$ 个位置，可以分解上式中的  $F\left(y_{t-1}, y_{t}, x_{1: T}\right)$ 为 2 个部分，即：$x_{1: T}$ 对 $y_{t}$ 的影响以及 $y_{t-1}$ 、 $y_{t}$ 间的影响。数学化表示为:
$$F\left(y_{t-1}, y_{t}, x_{1: T}\right)=\triangle y_{t}, x_{1: T}+\triangle y_{t-1}, y_{t}, x_{1: T}$$
其中，  $\triangle y_{t}, x_{1: T}$ 为状态函数，即表示为在 $t$ 位置上的节点 $y_{t}$ 状态；$\triangle y_{t-1}, y_{t}, x_{1: T}$ 为转移函数，即表示当前节点 $y_{t}$ 与上一个节点 $y_{t-1}$ 的相关性。我们将上面的分解用另一种方式来叙述，第一部分实际上是定义在 $𝑌$ 节点上的节点特征函数，这类特征函数只和当前节点有关，我们将其记为$s_{l}\left(y_{i}, X, i\right), l=1,2, \ldots, L$，第二部分则为 $𝑌$ 节点上的局部特征函数，这类特征函数只和当前节点和上一个节点有关，我们将其记为 $t_{k}\left(y_{i-1}, y_{i}, X, i\right), k=1,2, \ldots, K$。

需要注意的是，无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设 $t_{k}$ 的权重系数是  $\lambda_{k}$, $s_{l}$ 的权重系数是 $\mu_{l}$ ，则linear-CRF的参数化形式为:
$$P(Y \mid X)=\frac{1}{Z(X)} \exp \sum_{i=1}^{T}\left(\sum_{k=1}^{K} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, X, i\right)+\sum_{l=1}^{L} \mu_{l} s_{l}\left(y_{i}, X, i\right)\right)$$
$Y$  表示的是标注序列，是一个列向量，长度为  $T$  ；  $X$ 表示的词语序列，也是一个列向量，长度也为 $T$ 。其中， $Z(X)$ 为规范化因子:
$$Z(X)=\sum_{Y} \exp \sum_{i=1}^{T}\left(\sum_{k=1}^{K} \lambda_{k} t_{k}\left(y_{i-1}, y_{i}, X, i\right)+\sum_{l}^{L} \mu_{l} s_{l}\left(y_{i}, X, i\right)\right)$$
## 2. 示例

对应于书中的第一个例子，假设输入的观测序列 $X=\left(x_{1}, x_{2}, x_{3}\right)$  ，输出的词性标记为 $Y=\left(y_{1}, y_{2}, y_{3}\right)$ 。其中， $\forall y_{i} \in Y, y_{i} \in 1  (名词), 2(动词)$
$$\begin{array}{l}
t_{1}=t_{1}\left(y_{i-1}=1, y_{i}=2, x, i\right), i=2,3 \quad \lambda_{1}=1.0\\
t_{2}=t_{2}\left(y_{1}=1, y_{2}=1, x, 2\right) \quad \lambda_{2}=0.5\\
t_{3}=t_{3}\left(y_{2}=2, y_{3}=1, x, 3\right) \quad \lambda_{3}=1.0\\
t_{4}=t_{4}\left(y_{1}=2, y_{2}=1, x, 2\right) \quad \lambda_{4}=1.0\\
t_{5}=t_{5}\left(y_{2}=2, y_{3}=2, x, 3\right) \quad \lambda_{5}=0.2\\
s_{1}=s_{1}\left(y_{1}=1, x, 1\right) \quad \mu_{1}=1.0\\
s_{2}=s_{2}\left(y_{i}=2, x, i\right), i=1,2 \quad \mu_{2}=0.5\\
\begin{array}{ll}
s_{3}=s_{3}\left(y_{i}=1, x, i\right), i=2,3 & \mu_{3}=0.8 \\
s_{4}=s_{4}\left(y_{3}=2, x, 3\right) & \mu_{4}=0.5
\end{array}\\
s_{4}=s_{4}\left(y_{3}=2, x, 3\right) \quad \mu_{4}=0.5
\end{array}$$
于是线性链条件随机场模型就成为：
$$P(y \mid x) \propto \exp \left[\sum_{k=1}^{5} \lambda_{k} \sum_{i=2}^{3} t_{k}\left(y_{i-1}, y_{i}, x, i\right)+\sum_{k=1}^{4} \mu_{k} \sum_{i=1}^{3} s_{k}\left(y_{i}, x, i\right)\right]$$
于是对于标记 $(1, 2, 2)$ 的非规范化概率则为：
$$\begin{aligned}
P\left(y_{1}=1, y_{2}=2, y_{3}=2 \mid x\right) & \propto \exp \left(\lambda_{1}+\lambda_{5}+\mu_{1}+\mu_{2}+\mu_{4}\right) \\
& \propto \exp (3.2)
\end{aligned}$$
如果要求规范化概率，我们需要除以规范化因子 $Z(X)$ ，对于长度为3的标记序列而言， $Z(X)$ 是将8种可能的概率做加和，这便是全局归一化。

## 3. CRF的简化形式

我们可以发现，前面的式子如果将句子长度增加到一定程度，公示的长度将超出我们的预期很多倍，因此我们可以将前面的公式用更简便的方式书写，将同一特征在各个位置求和，将局部特征函数转为全局特征函数，首先设我们有 $K_{1}$ 个局部特征函数 $t_{k}$，$K_{2}$ 个节点特征函数 $s_{l}$ ，于是我们共有  $K=K_{1}+K_{2}$  个特征函数，用特征函数  $f_{k}\left(y_{i-1}, y_{i}, X, i\right)$来统一表示 $t_k$ 与 $s_l$：
$$f_{k}\left(y_{i-1}, y_{i}, x, i\right)=\left\{\begin{array}{ll}
t_{k}\left(y_{i-1}, y_{i}, x, i\right), & k=1,2, \cdots, K_{1} \\
s_{l}\left(y_{i}, x, i\right), & k=K_{1}+l ; l=1,2, \cdots, K_{2}
\end{array}\right.$$
对  $f_{k}\left(y_{i-1}, y_{i}, X, i\right)$  在各个序列位置 $i$ 求和得到:
$$f_{k}(Y, X)=\sum_{i=1}^{T} f_{k}\left(y_{i-1}, y_{i}, X, i\right)$$
同时也统一  $f_{k}\left(y_{i-1}, y_{i}, x, i\right)$  对应的权重系数 $w_{k}$ :
$$w_{k}=\left\{\begin{array}{lr}
\lambda_{k} & k=1,2, \ldots, K_{1} \\
\mu_{l} & k=K_{1}+l, l=1,2, \ldots, K_{2}
\end{array}\right.$$
于是就有：
$$P(Y \mid X)=\frac{1}{Z(X)} \exp \sum_{k=1}^{K} w_{k} f_{k}(Y, X)$$
其中，规范化因子:
$$Z(X)=\sum_{Y} \exp \sum_{k=1}^{K} w_{k} f_{k}(Y, X)$$

如果对 $f_{k}(Y, X)$ 和 $w_{k}$ 进行向量化表示， $F(Y, X)$ 和 $W$ 都是 $K \times 1$ 的列向量:
$$\begin{aligned}
W &=\left[\begin{array}{c}
w_{1} \\
w_{2} \\
\ldots \\
w_{K}
\end{array}\right], \text{  }
F(Y, X)= {\left[\begin{array}{c}
f_{1}(Y, X) \\
f_{2}(Y, X) \\
\ldots \ldots \ldots \\
f_{K}(Y, X)
\end{array}\right] }
\end{aligned}$$

那么Linear-CRF的向量内积形式可以表示为:
$$\begin{aligned}
P_{W}(Y \mid X) &=\frac{\exp (W \bullet F(Y, X))}{Z(X, W)} \\
&=\frac{\exp (W \bullet F(Y, X))}{\sum_{Y} \exp (W \bullet F(Y, X))}
\end{aligned}$$

# 三、CRF要解决的三类概率计算问题

本节主要解决以下三类问题：
- Inference: 计算条件概率分布，即给定条件随机场和输入输出序列，算出序列中每个位置所对应标注的概率，即给定$P\left(Y \mid X\right)$，计算 $P\left(y_{t}=i \mid X\right)$
- Learning: 把参数学习出来 (parameter estimation)，也就是给定 $N$ 个训练数据，求上面向量表示的 $W$ 的参数值，即: $\hat{W}=\operatorname{argmax} \prod_{i=1}^{N} P\left(Y^{(i)} \mid X^{(i)}\right)$
- Decoding: 给定X序列，找到一个最有可能的标注序列，即：$\hat{Y}=\operatorname{argmax} P(Y \mid X)$ ，其中， $Y=y_{1} y_{2} \ldots y_{T}$ 

## 1. Inference

对于第一类问题，CRF模型和隐马尔可夫模型一样，都引入前向-后向向量，递归地计算概率和期望值，对应的结果为
$$P\left(y_{t}=i \mid X\right)=\frac{1}{Z(X)} \alpha_{t}\left(y_{t}=i \mid X\right) \cdot \beta_{t}\left(y_{t}=i \mid X\right)$$
其中前向递推公式为：
$$\alpha_{t}\left(y_{t}=i \mid X\right)=\sum_{j \in S}\left(\psi_{t}\left(y_{t-1}=j, y_{t}=i, X\right) \cdot \alpha_{t-1}\left(y_{t-1}=j \mid X\right)\right)$$
后向递推公式为：
$$\beta_{t}\left(y_{t}=i \mid X\right)=\sum_{j \in S}\left(\psi_{t}\left(y_{t}=i, y_{t+1}=j, X\right) \cdot \beta_{t+1}\left(y_{t+1}=j \mid X\right)\right)$$
具体的思路其实是以 $y_t$ 为中心，将序列Y切分成了左右两段：
$$\begin{aligned}
P\left(y_{t}=i \mid X\right) &=\frac{1}{Z(X)} \Delta_{\text {左 }} \cdot \triangle_{\text {右 }} \\
\triangle_{\text {左 }} &=\sum_{y<1: t-1>} \psi_{1}\left(y_{0}, y_{1}, X\right) \cdot \psi_{2}\left(y_{1}, y_{2}, X\right) \cdot \ldots \psi_{t}\left(y_{t-1}, y_{t}=i, X\right) \\
\triangle_{\text {右 }} &=\sum_{y<t+1: T>} \psi_{t}\left(y_{t}, y_{t+1}, X\right) \cdot \psi_{t+1}\left(y_{t+1}, y_{t+2}, X\right) \cdot \ldots \psi_{T}\left(y_{T-1}, y_{T}, X\right)
\end{aligned}$$
然后对 $\triangle_{\text {左 }}$和 $\triangle_{\text {右 }}$分别做简化就得到了我们的 $\alpha_t$ 和 $\beta_t$，具体的计算可以查看原书。

## 2. Learning

条件随机场的训练过程和其他模型用到的优化算法都一样，能用在log-linear models上的求参方法都可以在这里使用，参见原书即可。


## 3. Decoding

Decoding也常称为条件随机场的预测，也就是给定条件随机场 $P(Y|X)$ 和输入序列 $x$，求使得条件概率最大的输出序列 $y^*$，条件随机场的预测算法就是著名的维特比算法（Vitebi）。

![vitebi_0](https://cdn.staticaly.com/gh/chenkai66/Picture@main/vitebi_0.32gadmqby2g0.webp)

由上图可知，我们需要做的就是找到一个最优路径使得概率最大，而如果遍历计算复杂度太高。维特比算法本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。
$$\begin{aligned}
y^{*} &=\arg \max _{y} P_{w}(y \mid x) \\
&=\arg \max _{y} \frac{\exp (w \cdot F(y, x))}{Z_{w}(x)} \\
&=\arg \max _{y} \exp (w \cdot F(y, x)) \\
&=\arg \max _{y}(w \cdot F(y, x))
\end{aligned}$$
求解上式相当于求解非规范概率最大的最优路径问题，也即：
$$\max _{y}\sum_{i=1}^n(w \cdot F_i(y_{i-1}, y_i, x))$$
其中
$$F_{i}\left(y_{i-1}, y_{i}, x\right)=\left(f_{1}\left(y_{i-1}, y_{i}, x, i\right), f_{2}\left(y_{i-1}, y_{i}, x, i\right), \cdots, f_{K}\left(y_{i-1}, y_{i}, x, i\right)\right)^{\mathrm{T}}$$
是局部特征向量。下面关于具体的算法部分我们举一个例子来对书中内容进行补充。
![vitebi](https://cdn.staticaly.com/gh/chenkai66/Picture@main/vitebi.2a9pqw7ljrms.webp)

例如对于这张图中，我们要求从S到E的最短路径，我们分为三步来做：
1. 首先求出S到A层四个节点的距离，这些距离都是S到A层各节点的最短距离；
2. 其次求S到B层四个节点的最短距离，我们将通过S到A层各节点的距离加上A层各节点到B层的距离得到，运算一共是 $4\times 4=16$ 次；
3. 这一步要求S到C层四个节点的最短距离，我们依然会利用S到B层的距离，但需要注意到，尽管第二步中我们有16个结果，我们只需要保留S到4个B节点的最短路径和距离即可，因此要求S到C的距离我们同样只需要运算 $4\times 4=16$ 次；
4. 最后一步顺理成章得到S到E的最短距离啦！
