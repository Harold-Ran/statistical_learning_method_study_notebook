# 第5章 决策树

在正式讲决策树之前，不知道大家有没有听过20提问的游戏，这个游戏的规则是，由出题人写下一个答案，提问者向出题人提出20个问题来猜答案是什么，对于提问者提出的问题，出题人只能回答“是”或“不是”。

我们今天就来简易地模拟这个游戏，通过4次提问来找到答案：

1. 提问：是生物吗？  答：是
2. 提问：是生活在水中的吗？  答：不是
3. 提问：是常见的宠物吗？  答：是
4. 提问：是狗吗？  答：是

bingo，在第四次的提问中你找到了答案，出题人写的正是狗。在上述的游戏模拟中你通过提问来获得更多的信息，再根据出题人的回答来确定答案所在的范围，把你在游戏中的思考决策方式用树的形式展现出来就如下图所示：

<img src="images\chap5-提问游戏决策树.png" alt="chap5-提问游戏决策树" style="zoom:100%;" />

这就是一棵用于分类的决策树，“正确答案”和“错误答案”就是你要预测的分类标签，而你提出的问题就是答案的特征。

通过这棵模拟游戏的决策树你会发现，要通过决策树找到正确的答案需要三个步骤：

1. 特征选择：找到一个合适的特征，即提出一个合适的问题来快速缩小决策范围
2. 决策树生成：通过多个特征和对应的标签，即多次提问和对应的回答来形成一棵决策树
3. 决策树剪枝：去掉多余的特征，比如这里出题人的答案是狗，我们就不需要再增加一个“是拉布拉多吗”这样的提问。

## 1 特征选择

我们希望每次都能找到一个特征，使决策的范围最大程度地缩小，获取最多的有效信息，而有效信息增加了多少要怎么来衡量呢？答案是信息增益。

在信息论与概率统计中，用熵来表示不确定程度，熵越大就越不确定。在机器学习中，训练集往往不能包括所有可能的数据，只能通过有限的数据来估计一个最有可能的概率，由此计算出的熵称为经验熵。

那么对于一个训练数据集D，在不知道任何特征的情况下，它的经验熵为
$$
\begin{aligned}
H(D)
&=-\sum_{k=1}^Kp_k\log_2p_k \\
&=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}
\end{aligned}
$$
式中，$p_k$是训练数据集D中样本标签为$k$的概率，在训练数据足够的情况下，我们知道$p_k$最有可能的值就是样本中标签为$k$的个数$|C_k|$除以整个数据集的样本个数$|D|$，即$\frac{|C_k|}{|D|}$。

当我们知道了特征A之后，也就是在开篇的模拟游戏中提出了一个问题并得到了确定的回答，在这种条件下数据集的经验熵就变成了
$$
\begin{aligned}
H(D|A)
&=\sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i) \\
&=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|}
\end{aligned}
$$
上式可以这么理解，在已知了特征A后，数据集的所有样本就可以根据特征A的n个取值（比如游戏中回答是和不是）划分到n个子集中去（比如游戏中根据回答可以把决策范围划分为两个子集，一个回答为“是”的子集，另一个回答为“不是”的子集），对于取值$i$，其对应的子集用$D_i$来表示，那么样本的特征A取值为$i$的概率估计就是子集$D_i$的样本个数$|D_i|$除以整个数据集的样本个数$|D|$，即$\frac{|D_i|}{|D|}$，于是已知特征A的条件下整个数据集的经验熵$H(D|A)$就是每个子集$D_i$的经验熵$H(D_i)$的加权之和，权重就是样本在子集$D_i$中的概率$\frac{|D_i|}{|D|}$。

在已知了特征A之后，有效信息增加了，不确定性减少了，因此由特征A带来的信息增益$g(D,A)$就是在已知特征A后不确定性减少的值，即
$$
g(D,A)=H(D)-H(D|A)
$$
有了一个可以量化的衡量标准信息增益，那么要选择合适的特征就很容易了，只需要把所有特征的信息增益计算出来，每次选择信息增益最大的那个就可以快速锁定答案了。

但是在实际操作的过程中我们发现一个问题，取值个数多的特征信息增益往往会更大。但是特征取值个数多却并不一定能使我们快速锁定答案，比如假设我们在开篇的模拟游戏中增加一个提问：是什么颜色？回答可以是红色、黄色、绿色、其他，这个可以有多种回答的提问并不会给我们找到答案带来更多的好处。解决这个问题的方法是对信息增益进行校正，用信息增益除以数据集关于特征A的取值产生的熵$H_A(D)$，称为信息增益比$g_R(D,A)$，当特征A取值越多时，$H_A(D)$也会增大，对信息增益的惩罚就越大。
$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)} \\
H_A(D)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}
$$

## 2 决策树生成

明确了特征选择的准则之后，我们就可以以此为依据生成决策树。

ID3算法是一种经典的决策树生成算法，它以信息增益作为特征选择的依据。首先以整个训练集D作为根节点，计算每个特征的信息增益，从中选择信息增益最大的特征A，根据特征A的$n$个取值将训练集的样本划分到$n$个子集$D_1, D_2, ... D_n$中，然后以每个子集$D_i$作为结点，继续选择该结点下信息增益最大的特征，如此递归下去直到子集中所有样本的分类标签相同或是所有可选择的特征信息增益都小于某个设定的阈值，此时该结点就称为叶子结点，无需再往下划分了。

![chap5-决策树的生成](C:\Users\zengh\Desktop\统计学习方法读书笔记\chap5-决策树的生成.png)

 

C4.5算法与ID3算法相似，但是在特征选择上，它以信息增益比作为选择的依据，是ID3算法的改进。

## 3 决策树剪枝

从决策树的生成算法中可以看出，决策树在生成的时候并不考虑树的复杂度和泛化能力，只要还有可选择的特征且结点的样本不都是同一个分类标签它就会尽可能地往下划分继续生成子树。这就会导致决策树过拟合，对训练集学习得过于精确而一旦给出新的数据表现就往往不如人意。正如开篇的模拟游戏中我们希望决策树学习到所有的宠物狗都是正确答案，而不是精确地学习到拉布拉多是正确答案，对于金毛或是其他的宠物狗确不能识别为正确答案。

我们希望决策树的预测误差小，这是对训练集而言的，即能够很好地学习并正确分类训练样本；同时我们希望树的复杂度更低，这是对测试集而言的，即对于没有见过的测试数据也能有很好的预测能力。于是对决策树T可以定义这样一个损失函数
$$
\begin{aligned}
C_{\alpha}(T)
&=C(T)+\alpha|T| \\
&=\sum_{t=1}^{|T|}N_tH_t(T) + \alpha|T| \\
&=-\sum_{t=1}^{|T|}N_t\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t} + \alpha|T| \\
&=-\sum_{t=1}^{|T|}\sum_{k=1}^KN_{tk}\log\frac{N_{tk}}{N_t} + \alpha|T|
\end{aligned}
$$
式中$C(T)$是预测误差，其中$N_t$是叶子结点$t$中的样本个数，$H_t(T)$是叶子结点$t$的经验熵，即为样本划分到叶子结点$t$上的不确定程度，因此预测误差可以理解为所有训练集样本经过决策树预测后的不确定性总和。式中第二项$\alpha|T|$是对决策树复杂度的惩罚，其中$\alpha$是设定的惩罚系数，$|T|$是决策树叶子结点的个数，叶子结点越多就表明决策树越深越复杂。

有了可量化的损失函数之后，我们就可以以此为依据对决策树进行优化剪枝，目标是找到一个损失函数最小的决策树。

ID3和C4.5算法的剪枝是自下而上的，从叶子结点开始剪，对它的父结点不再划分，此时父结点就成为一个叶子结点，这样得到一棵新的决策树$T_A$，假设剪枝前的决策树为$T_B$，分别计算$T_A$和$T_B$的损失函数，如果剪枝后损失函数减小了，那说明剪枝后的决策树更好。由此不断地向上剪枝，直到不能再修剪了或是损失函数不再减小了，此时的决策树就是损失函数最小的决策树。

## 4 CART算法

CART全称是Classification and Regression Tree，从名字来看就包括分类树和回归树两种模型。相较于ID3和C4.5算法，CART算法最大的特点就在于，只生成二叉树，就像是我们开篇的模拟游戏，每次提问的回答只能是“是”和“不是”。

### 4.1 回归树生成

CART回归树选择特征的依据是平方误差最小。

对于训练集D，第$j$个特征其中一个取值为$s$，根据样本的第$j$个特征取值是否大于$s$可以把样本划分为$R_1(j,s)=\{x|x^{(j)}\leq s\}$和$R_1(j,s)=\{x|x^{(j)}>s\}$两部分。容易知道，对于一个样本子集，预测标签为子集中所有样本标签的均值时，这个子集的平方误差是最小的。因此，$R_1(j,s)$的预测标签为其中所有样本标签的均值$\hat{c}_1=ave(y_i|x_i\in R_1(j,s))$，同样的，$R_2(j,s)$的预测标签为$\hat{c}_2=ave(y_i|x_i\in R_2(j,s))$，此时，以特征$j$的取值$s$划分样本的平方误差就是$R_1(j,s)$和$R_2(j,s)$两部分的平方误差之和
$$
\sum_{x_i\in R_1(j,s)}(y_i-\hat{c}_1)^2 + \sum_{x_i\in R_2(j,s)}(y_i-\hat{c}_2)^2
$$
计算出所有可选择的特征的所有可能的取值的平方误差，从中选择平方误差最小的那个特征$j$和取值$s$，如此递归下去直到满足停止条件，这样就得到了一棵CART回归树。这里的停止条件是我们设定的树的深度、叶子结点个数等超参数。

CART回归树的生成过程有点像我们玩猜数字游戏的决策逻辑。例如我们要从整数1~100中猜一个数字，那么我们首先会猜中间数50，这样我们可以根据回答“大了”或“小了”把猜测范围缩小一半，如果我们一开始猜20，那么运气好的话我们能把猜测范围缩小80%，但是运气不好的话就只能缩小20%，因此我们每次都把猜测范围划分成均等的两部分，根据回答舍沿着其中一部分不断猜下去，直到猜中答案。

![chap5-猜数字游戏决策树](C:\Users\zengh\Desktop\统计学习方法读书笔记\chap5-猜数字游戏决策树.png)

### 4.2 分类树生成

不同于ID3和C4.5算法，CART分类树的特征选择依据是基尼指数。假设训练数据集D有K个分类标签，样本属于第$k$类的概率是$p_k$，那么训练集D的基尼指数为
$$
\begin{aligned}
Gini(D)
&=\sum_{k=1}^Kp_k(1-p_k) \\
&=1-\sum_{k=1}^Kp_k^2 \\
&=1-\sum_{k-1}^K(\frac{|C_k|}{|D|})
\end{aligned}
$$
式中$|C_k|$为训练集中属于第$k$类的样本个数。

在以特征A的某一取值$s$将训练集划分为$D_1$和$D_2$两部分的条件下，基尼指数为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_1)
$$
基尼指数和熵都可以衡量不确定程度，但是基尼指数相较于熵更易于计算。同时，基尼指数不受特征取值个数多少的影响，它的作用类似于信息增益比，但不同的是，基尼指数$Gini(D,A)$越小代表由于特征A使得分类的不确定性越小，此时特征越好，而信息增益比是越大特征越好，这一点上二者是相反的。

因此，CART分类树在生成时，每次计算出所有可选择的特征及取值$s$的基尼指数，从中选择基尼指数最小的作为最优特征和切分点，如此递归下去直到满足停止条件。

### 4.3 CART剪枝

CART剪枝算法与ID3和C4.5剪枝算法也是不同的，在ID3和C4.5剪枝算法中，$\alpha$是一个人工设定的超参数，而CART剪枝算法中，$\alpha$是与子树相对应的参数。

具体来说，对于生成的整个决策树$T_0$，以其中的结点$t$为一棵单结点决策树时，它的损失函数$C_{\alpha}(t)$为
$$
C_{\alpha}(t)=C(t)+\alpha|t|=C(t)+\alpha
$$
以结点$t$为根结点的子树$T_t$，它的损失函数$C_{\alpha}(T_t)$为
$$
C_{\alpha}(T_t)=C(T_t)+\alpha|T_t|
$$
当$\alpha$较小时，损失函数对决策树复杂度的惩罚较小，而子树$T_t$的预测能力必然好于单结点树$t$，因此有$C_{\alpha}(T_t)<C_{\alpha}(t)$，但是当$\alpha$增大到某一值时，二者的损失函数相等，可以计算出此时的参数$\alpha$为
$$
\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}
$$
两损失函数相等时表明单个结点$t$和子树$T_t$的损失是相同的，而子树$T_t$比单结点树$t$拥有的结点更多更复杂，既然二者损失相同，那么不如剪去子树$T_t$，只保留结点$t$作为叶子结点，剪去$T_t$后的决策树就是在参数$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$的情况下的最优子树。

![chap5-CART剪枝](C:\Users\zengh\Desktop\统计学习方法读书笔记\chap5-CART剪枝.png)

依照这个思路，我们可以对决策树$T_0$中的每个结点$t$都计算出对应的参数$\alpha$，用$g(t)$表示，取$g(t)$的最小值为$\alpha_1$，对应的剪枝后的最优子树为$T_1$，然后对$T_1$的每个内部结点$t$继续计算对应的参数$\alpha$，找到其中的最小值作为$\alpha_2$，对应的最优子树为$T_2$，如此递归地进行剪枝，直到只剩下根结点或只剩下根结点和两个叶子结点，此时我们就得到了$n$个最优子树$T_1, T_2, ..., T_n$，可以采用交叉验证的方法从中选择一个预测效果最好的子树$T_{\alpha}$作为最终的决策树模型。

