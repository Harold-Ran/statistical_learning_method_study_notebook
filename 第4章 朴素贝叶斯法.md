# 第4章 朴素贝叶斯法

朴素贝叶斯法，一看到这个名称就会自然而然地想到贝叶斯定理。那么朴素贝叶斯法和贝叶斯定理有什么关系呢？它又为什么朴素呢？别着急，听我细细讲来。

## 朴素贝叶斯法因何朴素？

对于一个训练数据集$T=\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\}$，它的每一条数据都是依据同样的规则产生的，用数学语言来表达就是每一条数据依$P(X, Y)$独立同分布产生，而机器学习就是要学习到这种规则，然后依据学到的规则去预测新的数据。朴素贝叶斯法瞄准的学习目标就是$P(X, Y)$，想法非常的简单明了，当我们知道了这份数据集是怎么产生的，那么对于一个新的输入数据$x$，要预测它的标签$y$自然就并非难事了，这种直接学习数据的生成方式的模型就称为生成模型。

要怎么学习联合分布$P(X, Y)$呢？朴素贝叶斯法决定分三步走：

1) 首先学习数据集中标签$Y$取值为$c_k$的概率

$$
P(Y=c_k),\quad k=1, 2, ..., K
$$

2. 然后学习标签$Y$取值为$c_k$的条件下输入$X$为向量$x$的概率

$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)}|Y=c_k)
$$

3. 最后根据条件概率公式就可以得到联合概率

$$
P(X=x,Y=c_k)=P(X=x|Y=c_k)P(Y=c_k)
$$

只需三步就达成了目标，看起来如此容易，做起来却困难重重。假设输入$X$的每个特征$x^{(j)}$的可能取值有$S_j$个，标签$Y$的可能取值有$K$个，那么一个特征向量$x=(x^{(1)}, ..., x^{(n)})$可能的组合就有$\prod_{j=1}^nS_j$个，所要学习的条件概率$P(X=x|Y=c_k)$就有$K\prod_{j=1}^nS_j$个，难度是指数级的！

然而，我们的前辈们是不会因为这点困难就放弃的，既然最大的问题在于条件概率，那不如就从此入手。于是，他们提出了一个朴素的假设：特征之间是条件独立的。

基于这个假设，条件概率就可以写为
$$
\begin{aligned}
P(X=x|Y=c_k)
&=P(X^{(1)}=x^{(1)}, ..., X^{(n)}=x^{(n)}|Y=c_k) \\
&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)
\end{aligned}
$$
此时，要计算条件概率就只需要知道标签$Y$取值为$c_k$的条件下输入$X$的每个特征取值为$x^{(j)}$的概率$P(X^{(j)}=x^{(j)}|Y=c_k)$，这个条件概率的个数是$K\sum_{j=1}^nS_j$，从连乘变为连加，问题瞬间就简化了。

因为这个朴素而强大的假设，朴素贝叶斯法由此得名。然而有得就有失，这个朴素的假设让朴素贝叶斯法变得简单且易于实现，但同时也限制了它的应用，因为在现实世界中，特征之间往往相互关联，不能满足条件独立性假设，朴素贝叶斯法也就难以获得理想的预测效果。

不论如何，朴素贝叶斯法仍然是一众分类算法中高效且学习效果较好的分类器之一，在文本分类、垃圾邮件分类等问题中有着广泛应用。

## 朴素贝叶斯法与贝叶斯定理

基于条件独立性这个朴素的假设，可以得到训练数据集的生成方式$P(X, Y)$，但我们的终极目标是要进行预测，也就是要得到在给定一个新的输入$X=x$的条件下，它对应的标签$Y$的取值是什么。

这时就轮到朴素贝叶斯法名字中的另一个主角——贝叶斯定理出场了，通过前面的三步我们已经得到了$P(Y=c_k)$和$P(X=x|Y=c_k)$，利用贝叶斯定理就可以得到
$$
\begin{aligned}
P(Y=c_k|X=x)
&=\frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)} \\
&=\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}, \quad k=1, 2, ..., K
\end{aligned}
$$
后验概率$P(Y=c_k|X=x)$是在给定输入$X=x$条件下标签$Y$取值为$c_k$的概率，当我们利用贝叶斯定理计算得到每一个标签$Y$的可能取值$c_1, c_2, ..., c_K$的条件概率时，可以很自然的想到用最有可能（概率最大）的那个标签取值作为预测结果是最合理的，这就是后验概率最大化的思想。

用数学公式来表示，朴素贝叶斯法可以写为
$$
\begin{aligned}
y=f(x)
&=\arg \max_{c_k}P(Y=c_k|X=x) \\
&=\arg \max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)}, \quad k=1, 2, ..., K
\end{aligned}
$$
因为上式对标签的每个可能取值$c_k$的分母都一样，只需要取分子最大的那个标签取值就可以，于是可以写为
$$
\begin{aligned}
y=f(x)
&=\arg \max_{c_k}P(Y=c_k)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k), \quad k=1, 2, ..., K
\end{aligned}
$$
这就是朴素贝叶斯法的数学表达。

## 朴素贝叶斯法的参数估计方法

说了这么多，朴素贝叶斯法的参数$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$究竟要怎么学习到呢？这里就要用到参数估计方法。为啥是估计呢，因为训练数据集只是依概率$P(X, Y)$独立同分布产生的所有可能的数据的一部分，是样本而非总体，从样本中学习到的参数并非总体参数，当训练数据集的样本达到一定数量时，从样本中学习到的参数与总体参数之间的误差就会控制在我们可接受的范围内，这个概念是我们在学习统计学习方法的时候需要厘清的。

最简单的参数估计方法是采用极大似然估计。极大似然估计的原理用一句话来说就是，根据已知的样本找到参数最有可能（概率最大）的取值。

假设样本数量足够，那么参数$P(Y=c_k)$最有可能的取值就是标签$Y$取值为$c_k$的样本数量除以样本总量，写成数学表达式就是
$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N}
$$
条件概率$P(X^{(j)}=x^{(j)}|Y=c_k)=\frac{P(X^{(j)}=x^{(j)}, Y=c_k)}{P(Y=c_k)}$，其中$P(Y=c_k)$的估计值通过上式已经得到了，联合概率$P(X^{(j)}=x^{(j)}, Y=c_k)$最有可能的取值就是第$j$个特征取值为$x^{(j)}$（书中用符号$a_{jl}$来表示第$j$个特征的取值）且标签$Y$取值为$c_k$的样本数量除以样本总量，于是可以得到
$$
\begin{aligned}
P(X^{(j)}=x^{(j)}|Y=c_k)
&=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)}， y_i=c_k)}{N}/\frac{\sum_{i=1}^NI(y_i=c_k)}{N} \\
&=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)}， y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
\end{aligned}
$$
极大似然估计虽然简单，但是会存在一个问题，就是估计的参数值为零，使得分类结果不正确。例如，训练数据集中没有第$j$个特征取值为$x^{(j)}$且标签$Y$取值为$c_k$的样本，则估计得到的条件概率$P(X^{(j)}=x^{(j)}|Y=c_k)$为零，根据朴素贝叶斯法的数学表达式，对于标签$Y$的每一个可能的取值$c_k$，后验概率$P(Y=c_k|X=x)$都为零，此时预测的标签是什么呢？显然这种情况下得不到正确的分类结果。

解决的办法是向分子中加入一个正数$\lambda$来保证估计的参数不为零，同时为了保证估计的参数是一种概率分布（要满足概率之和为1），需要对分母也进行修改，这种方法称为贝叶斯估计。

在采用贝叶斯估计的情况下，参数的表达式就写为
$$
P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda} \\
\begin{aligned}
P(X^{(j)}=x^{(j)}|Y=c_k)
&=\frac{\sum_{i=1}^NI(x_i^{(j)}=x^{(j)}， y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}
\end{aligned}
$$
